{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ac7f316-57c0-418b-bc30-7193b7bd5317",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# To work with Amazon S3 storage, set the following variables using your AWS Access Key and Secret Key\n",
    "# Set the Region to where your files are stored in S3.\n",
    "access_key = 'xxxxxxxxxxxxxx'\n",
    "secret_key = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "# Set the environment variables so boto3 can pick them up later\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = access_key\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = secret_key\n",
    "encoded_secret_key = secret_key.replace(\"/\", \"%2F\")\n",
    "aws_region = \"us-east-2\"\n",
    "\n",
    "# Update the Spark options to work with our AWS Credentials\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n",
    "\n",
    "sdf = spark.read.parquet('s3a://hvfhv-project-mc/landing/fhvhv_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88bea91b-8a6b-489a-b286-64282689dcbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import some functions we will need later on\n",
    "from pyspark.sql.functions import col, isnan, when, count, udf\n",
    "\n",
    "# Set the Spark logging level to only show errors\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0f16ee-95aa-443e-bb45-cee5e895b514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: 18479031"
     ]
    }
   ],
   "source": [
    "sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0b3d8b4-c2cc-4d9e-b6e6-826150467a6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- hvfhs_license_num: string (nullable = true)\n |-- dispatching_base_num: string (nullable = true)\n |-- originating_base_num: string (nullable = true)\n |-- request_datetime: timestamp (nullable = true)\n |-- on_scene_datetime: timestamp (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- PULocationID: long (nullable = true)\n |-- DOLocationID: long (nullable = true)\n |-- trip_miles: double (nullable = true)\n |-- trip_time: long (nullable = true)\n |-- base_passenger_fare: double (nullable = true)\n |-- tolls: double (nullable = true)\n |-- bcf: double (nullable = true)\n |-- sales_tax: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- tips: double (nullable = true)\n |-- driver_pay: double (nullable = true)\n |-- shared_request_flag: string (nullable = true)\n |-- shared_match_flag: string (nullable = true)\n |-- access_a_ride_flag: string (nullable = true)\n |-- wav_request_flag: string (nullable = true)\n |-- wav_match_flag: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1023aeb0-12dc-4860-b339-1f1179fbe126",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns (11)\n",
    "dropped_cols_sdf = sdf.drop('hvfhs_license_num', 'dispatching_base_num', 'originating_base_num', 'PULocationID', 'DOLocationID', 'driver_pay', 'shared_request_flag', 'shared_match_flag', 'wav_request_flag', 'wav_match_flag', 'access_a_ride_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faee4994-1bb5-4484-88d8-87a6c9661b05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- request_datetime: timestamp (nullable = true)\n |-- on_scene_datetime: timestamp (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- trip_miles: double (nullable = true)\n |-- trip_time: long (nullable = true)\n |-- base_passenger_fare: double (nullable = true)\n |-- tolls: double (nullable = true)\n |-- bcf: double (nullable = true)\n |-- sales_tax: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- tips: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Print schema of dropped_cols_sdf\n",
    "dropped_cols_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f82bacd-65bb-4f34-bcfa-ac08eb0257de",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|on_scene_datetime|\n+-----------------+\n|          4891992|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check how many records are null in the \"on_scene_datetime\" column\n",
    "dropped_cols_sdf.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"on_scene_datetime\"] ] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a44d235b-486c-46c9-a93c-b6fe894e248e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop null records from \"on_scene_datetime\" column\n",
    "clean_sdf = dropped_cols_sdf.na.drop(subset=[\"on_scene_datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32d836e3-7dce-4867-8191-3ffe183624e7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n|on_scene_datetime|\n+-----------------+\n|         13587039|\n+-----------------+\n\n+-----------------+\n|on_scene_datetime|\n+-----------------+\n|                0|\n+-----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Check how many records are null in the \"on_scene_datetime\" column\n",
    "clean_sdf.select([count(when(col(c).isNotNull(), c)).alias(c) for c in [\"on_scene_datetime\"] ] ).show()\n",
    "clean_sdf.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"on_scene_datetime\"] ] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "152c47e5-fd2c-4cc8-98ac-040e24af300d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- request_datetime: timestamp (nullable = true)\n |-- on_scene_datetime: timestamp (nullable = true)\n |-- pickup_datetime: timestamp (nullable = true)\n |-- dropoff_datetime: timestamp (nullable = true)\n |-- trip_miles: double (nullable = true)\n |-- trip_time: long (nullable = true)\n |-- base_passenger_fare: double (nullable = true)\n |-- tolls: double (nullable = true)\n |-- bcf: double (nullable = true)\n |-- sales_tax: double (nullable = true)\n |-- congestion_surcharge: double (nullable = true)\n |-- airport_fee: double (nullable = true)\n |-- tips: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "clean_sdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01f7673-f0d6-485f-a355-4f7082f029f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+---------------+----------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+\n|request_datetime|on_scene_datetime|pickup_datetime|dropoff_datetime|trip_miles|trip_time|base_passenger_fare|tolls|bcf|sales_tax|congestion_surcharge|airport_fee|tips|\n+----------------+-----------------+---------------+----------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+\n|               0|                0|              0|               0|         0|        0|                  0|    0|  0|        0|                   0|          0|   0|\n+----------------+-----------------+---------------+----------------+----------+---------+-------------------+-----+---+---------+--------------------+-----------+----+\n\n"
     ]
    }
   ],
   "source": [
    "clean_sdf.select([count(when(col(c).isNull(), c)).alias(c) for c in [\"request_datetime\", \"on_scene_datetime\", \"pickup_datetime\", \"dropoff_datetime\", \"trip_miles\", \"trip_time\", \"base_passenger_fare\", \"tolls\", \"bcf\", \"sales_tax\", \"congestion_surcharge\", \"airport_fee\", \"tips\"] ] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3b17c6c-2642-47da-851c-5e69e034e024",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write clean_sdf back to parquet file and save it in raw folder\n",
    "clean_sdf.write.parquet('s3://hvfhv-project-mc/raw/cleaned_fhvhv_tripdata_2023-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45777b0-9447-4a19-8cb4-ee00e980f28c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Drop unnecessary columns (10), and save it as dropped_cols_sdf. DONE\n",
    "# Drop all nulls from \"on_scene_datetime\" columns, and save it as clean_sdf. DONE\n",
    "# Write clean_sdf back to parquet file and save it in raw folder. DONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e663bf37-f76c-48a0-8f82-2ad0cf8b7c82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[25]: False"
     ]
    }
   ],
   "source": [
    "# Check that \"access_a_ride_flag\" was dropped\n",
    "myColumns=dropped_cols_sdf.columns\n",
    "\"access_a_ride_flag\" in myColumns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6492cf8e-8cba-45df-a252-4d00eb790083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# clean_sdf.select([count(when(isnan(c) | col(c).isNotNull(), c)).alias(c) for c in [\"originating_base_num\"]] ).show()\n",
    "# clean_sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"originating_base_num\"]] ).show()\n",
    "# new_sdf.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in [\"access_a_ride_flag\"]] ).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HVFHV PySpark - Data Cleaning",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
